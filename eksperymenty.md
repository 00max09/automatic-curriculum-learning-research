### Sokoban i trening PLR na 10k i 100k ukończalnych poziomach

Udało się minimalnie podnieść wyniki przy użyciu większego zbioru, 10k jest zbyt małe, 100k wydaje się wystarczająco i prawdopodobnie przesianie zbioru danych w jakiś sposób może być sposobem na lekkie poprawienie wydajności uczenia

### Maze i trening PLR na 7k i 70k ukończalnych poziomów

Tu też wydaje się że ogólne wyniki przy zbiorze 70k poziomów są lekko lepsze niż przy pełnym zbiorze danych (zbiór mniejszy znów okazuje się niewystarczający), nawet w momencie w którym odsiane została mniej niż połowa poziomów